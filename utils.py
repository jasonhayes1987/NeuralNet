# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f_gWhHibpUXe_i2PQWKtQdtPuaBR9bLw
"""

import numpy as np
import cupy as cp
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import matplotlib.animation as animation
from sklearn.metrics import confusion_matrix
from tensorflow.image import resize
from tensorflow import convert_to_tensor
import itertools
from glob import glob


def im2col(input_data, filter_h, filter_w, stride=1, pad=0, xp=np):
    """
    Rearranges image blocks into columns.
    Input:
      - input_data: shape (N, C, H, W)
      - filter_h, filter_w: filter height and width
      - stride: stride for the convolution
      - pad: amount of zero-padding
      - xp: either np (CPU) or cp (GPU)
    Returns:
      - col: 2D array of shape (N*out_h*out_w, C*filter_h*filter_w)
    """
    N, C, H, W = input_data.shape

    # Determine padding values
    if isinstance(pad, int):
        pad_top = pad
        pad_bottom = pad
        pad_left = pad
        pad_right = pad
    else:
        # Expect pad to be ((pad_top, pad_bottom), (pad_left, pad_right))
        (pad_top, pad_bottom), (pad_left, pad_right) = pad

    # Pad the input
    input_padded = xp.pad(input_data, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode='constant')

    # Compute output dimensions
    out_h = (H + pad_top + pad_bottom - filter_h) // stride + 1
    out_w = (W + pad_left + pad_right - filter_w) // stride + 1

    # Get strides for the padded array
    s0, s1, s2, s3 = input_padded.strides

    # Build shape and strides for as_strided
    shape = (N, C, out_h, out_w, filter_h, filter_w)
    strides = (s0, s1, s2 * stride, s3 * stride, s2, s3)
    cols = xp.lib.stride_tricks.as_strided(input_padded, shape=shape, strides=strides)

    # Rearrange dimensions so that each patch becomes a row
    cols = cols.transpose(0, 2, 3, 1, 4, 5).reshape(N * out_h * out_w, -1)
    return cols


def col2im(cols, input_shape, filter_h, filter_w, stride=1, pad=0, xp=np):
    """
    Converts column representation back into image blocks.
    Input:
      - col: 2D array from im2col with shape (N*out_h*out_w, C*filter_h*filter_w)
      - input_shape: shape (N, C, H, W) of the original input data
      - filter_h, filter_w: filter dimensions
      - stride, pad: convolution parameters. If pad is an int, symmetric padding is assumed.
                   If pad is a tuple of tuples, it should be ((pad_top, pad_bottom), (pad_left, pad_right)).
      - xp: either np or cp
    Returns:
      - An array with shape (N, C, H, W)
    """
    N, C, H, W = input_shape

    # Determine padding values
    if isinstance(pad, int):
        pad_top = pad
        pad_bottom = pad
        pad_left = pad
        pad_right = pad
    else:
        (pad_top, pad_bottom), (pad_left, pad_right) = pad

    # Compute output dimensions from im2col
    out_h = (H + pad_top + pad_bottom - filter_h) // stride + 1
    out_w = (W + pad_left + pad_right - filter_w) // stride + 1

    # Reshape cols to (N, out_h, out_w, C, filter_h, filter_w)
    cols_reshaped = cols.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)

    # Prepare an output array with padding
    H_padded = H + pad_top + pad_bottom
    W_padded = W + pad_left + pad_right
    img_padded = xp.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)

    # Instead of looping over the entire image, we only loop over the filter dimensions.
    for y in range(filter_h):
        y_end = y + stride * out_h
        for x in range(filter_w):
            # Accumulate the values from cols_reshaped into the appropriate region of img_padded.
            img_padded[:, :, y:y_end:stride, x:x + stride * out_w] += cols_reshaped[:, :, y, x, :, :]

    # Remove padding and return the original image dimensions.
    return img_padded[:, :, pad_top:H_padded - pad_bottom, pad_left:W_padded - pad_right]


# store functions used for processing data

def train_test_split(x, y, split=0.2):
    """
    returns x and y data split into training and testing sets according to split percentage passed
    Input:
        x: input data
        y: output data
    Return:
        x_train, y_train, x_test, y_test
    """
    
    split_index = int(np.floor(len(y)*split))
    x_train = x[split_index:]
    y_train = y[split_index:]
    x_test = x[:split_index]
    y_test = y[:split_index]
    
    return x_train, y_train, x_test, y_test

def build_metric_figure(metric_data, num_batches_per_epoch=None):
    """
    Builds a figure with one subplot per metric type (e.g., MSE, R2, etc.),
    plotting both Train and Validation curves on the same plot.
    The subplots are arranged in a grid that is computed dynamically.
    """
    # Group metrics by metric type.
    grouped = {}
    for key, values in metric_data.items():
        parts = key.split()  # e.g. "Train MSE" -> ["Train", "MSE"]
        if len(parts) < 2:
            continue
        data_type = parts[0]      # "Train" or "Validation"
        metric_name = " ".join(parts[1:])  # e.g. "MSE"
        if metric_name not in grouped:
            grouped[metric_name] = {}
        grouped[metric_name][data_type] = values

    num_metrics = len(grouped)
    # Determine grid layout: try to make it as square as possible.
    n_cols = int(np.ceil(np.sqrt(num_metrics)))
    n_rows = int(np.ceil(num_metrics / n_cols))
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))
    axes = np.array(axes).flatten()  # Flatten in case of multi-dimensional array

    for i, (metric_name, data) in enumerate(grouped.items()):
        ax = axes[i]
        for data_type, values in data.items():
            xvals = np.arange(len(to_numpy(values))) + 1  # Batch numbering starts at 1
            yvals = np.array(to_numpy(values))
            ax.plot(xvals, yvals, label=data_type)
        ax.set_title(metric_name)
        ax.set_xlabel("Batch")
        ax.legend()
        if num_batches_per_epoch is not None:
            def batch_to_epoch(x):
                return x / num_batches_per_epoch
            def epoch_to_batch(x):
                return x * num_batches_per_epoch
            secax = ax.secondary_xaxis('top', functions=(batch_to_epoch, epoch_to_batch))
            secax.set_xlabel("Epoch")
    
    # Remove any unused subplots if our grid is larger than needed.
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])
    
    # fig.tight_layout()
    return fig


# def animate(network, metrics, plot1, plot2, plot_loss, plot_accuracy, plot_learning_rate):
#     plt.cla()
    
#     for l in network.layers:
#         plot1.imshow(l.weights, cmap="seismic", aspect='auto')
#         plot2.imshow(np.expand_dims(l.bias, axis=1).T, cmap="seismic", aspect='auto')
        
#     # plot metric data
#     # plot loss    
#     # check if total loss exists in metric data (means network applies regularization)
#     if 'Total Loss' in network.metric_data:
#         plot_loss.plot(metrics['Total Loss'][:,0], label='total train', color='red', lw=1)
#         plot_loss.plot(metrics['Total Loss'][:,1], label='total validation', color='red', lw=1, alpha=0.5)
#         # plot regularizations if exist
#         if 'L2 Regularization' in network.metric_data:
#             plot_loss.plot(metrics['L2 Regularization'][:,0], label='L2 train', color='orange', lw=1)
#             plot_loss.plot(metrics['L2 Regularization'][:,1], label='L2 validation', color='orange', lw=1, alpha=0.5)
#         if 'L1 Regularization' in network.metric_data:
#             plot_loss.plot(metrics['L1 Regularization'][:,0], label='L1 train', color='purple', lw=1)
#             plot_loss.plot(metrics['L1 Regularization'][:,1], label='L1 validation', color='purple', lw=1, alpha=0.5)
#     plot_loss.plot(metrics['Sparse CXE'][:,0], label='loss train', color='blue', lw=1)
#     plot_loss.plot(metrics['Sparse CXE'][:,1], label='loss validation', color='blue', lw=1, alpha=0.5)
    
#     # plot accuracy
#     plot_accuracy.plot(metrics['Accuracy'][:,0], label='train', color='green', linewidth=1)
#     plot_accuracy.plot(metrics['Accuracy'][:,1], label='validation', color='green', alpha=0.3, linewidth=1)
    
#     # plot learning rate
#     plot_learning_rate.plot(metrics['Learning Rate'][:,0], label='train', color='red', linewidth=1)

def get_confusion_matrix(data:tuple, model:"Neural_Network"):
    """
    returns a confusion matrix generated from predictions made on images passed through an image data generator

    INPUTS
    data: tuple, tuple of (x,y)
    model: Neural_Network, model used to make predictions on images

    RETURNS
    cm: confusion matrix
    """
    print('Generating Confusion Matrix')
    # Unpack data tuple
    x,targets = data
    predictions = model.predict(x)
    predictions = model._xp.argmax(predictions, axis=1)
    # If device = GPU, convert arrays to NumPy using .get()
    if hasattr(predictions, "get"):
        predictions = predictions.get()
    if hasattr(targets, "get"):
        targets = targets.get()
    # targets = model._xp.argmax(y, axis=1)
    cm = confusion_matrix(targets, predictions)
    return cm

# def plot_confusion_matrix(confusion_matrix, classes, normalize = False, title = 'Confusion Matrix', cmap = plt.cm.Blues):
#     # Convert classes to a sequence by using range
#     classes = np.arange(classes)
#     if normalize:
#         confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]

#     plt.figure(figsize=(10,6))
#     plt.imshow(confusion_matrix, interpolation='nearest', cmap=cmap)
#     plt.title(title)
#     plt.colorbar()
#     tick_marks = classes
#     plt.xticks(tick_marks, classes, rotation=45)
#     plt.yticks(tick_marks, classes)

#     fmt = '.2f' if normalize else 'd'
#     thresh = confusion_matrix.max() / 2.
#     for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):
#         plt.text(j, i, format(confusion_matrix[i, j], fmt),
#                  horizontalalignment='center',
#                  color='white' if confusion_matrix[i, j] > thresh else 'black')
#     plt.tight_layout()
#     plt.ylabel('True label')
#     plt.xlabel('Predicted label')
#     plt.show()

def plot_confusion_matrix(confusion_matrix, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.coolwarm):
    # Convert classes to a sequence by using range
    classes = np.arange(classes)
    if normalize:
        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(10,6))
    plt.imshow(confusion_matrix, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = classes
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = confusion_matrix.max() / 2.
    for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):
        plt.text(j, i, format(confusion_matrix[i, j], fmt),
                 horizontalalignment='center',
                 color='white' if confusion_matrix[i, j] > thresh else 'black')
    
    # Calculate accuracy and add it to the plot
    accuracy = get_accuracy_from_confusion_matrix(confusion_matrix)
    # Add the accuracy text below the plot (using axes coordinates)
    plt.text(0.02, -0.1, f'Accuracy = {accuracy*100:.2f}%', transform=plt.gca().transAxes, fontsize=12)
    
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

def get_accuracy_from_confusion_matrix(confusion_matrix):
    return confusion_matrix.trace() / confusion_matrix.sum()

def to_tf_tensor(x, model):
    # Convert x to a TensorFlow tensor
    # If using GPU (i.e. x is a cupy array), convert to a numpy array first.
    if model._device == "GPU":
        x_np = model._xp.asnumpy(x)
    else:
        x_np = x
    return convert_to_tensor(x_np)

def from_tf_tensor(x, model):
    # Convert a TensorFlow tensor back to the model's array type (numpy or cupy)
    x_np = x.numpy()
    if model._device == "GPU":
        return model._xp.asarray(x_np)
    else:
        return x_np
    
def to_numpy(x):
    # Convert to numpy array if cupy array else return original
    # If x is a list, tuple, or dict, function will parse recursively to convert each item
    if isinstance(x, list):
        return [to_numpy(i) for i in x]
    elif isinstance(x, tuple):
        return tuple(to_numpy(i) for i in x)
    elif isinstance(x, dict):
        return {k: to_numpy(v) for k, v in x.items()}
    else:
        return x.get() if hasattr(x, "get") else x
    
class Normalizer:

    def __init__(self, device='CPU'):
        self._device = device
        if self._device == 'CPU':
            self._xp = np
        elif self._device == 'GPU':
            self._xp = cp

    def normalize(self, x):
        """Normalizes x to have 'mean' and 'std'

        Args:
            x (np|cp array): data to normalize
        """
        x = self._xp.asarray(x)
        self.mean = self._xp.mean(x, axis=0, keepdims=True)
        self.std = self._xp.std(x, axis=0, keepdims=True)
        x = (x-self.mean)/self.std
        return x
    
    def denormalize(self, x):
        return x * self.std + self.mean