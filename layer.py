# -*- coding: utf-8 -*-
"""layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N3Cyi7maf8zlqt5IT1dZ5d3lwVIshFlP
"""

import numpy as np
from scipy import signal
import cupy as cp
from cupyx.scipy import signal as c_signal
from utils import im2col, col2im

# base layer class
class Layer:
    def __init__(self, device='CPU'):
        self._device = device
        if self._device == 'CPU':
            self._xp = np
        elif self._device == 'GPU':
            self._xp = cp
        self.input = None
        self.output = None
        self._input_dims = None
        self._output_dims = None

    def forward(self, input, is_training=True):
      # computes output 'Y' of layer given input 'X'
      raise NotImplementedError("Must override 'forward' by instantiating child of layer class (Activation, Dense, Convolutional, etc...")

    def backward(self, output_gradient, learning_rate):
      # computes derivative of input 'X' of layer given output error 'dE/dY'
      raise NotImplementedError("Must override 'backward' by instantiating child of layer class (Activation, Dense, Convolutional, etc...")

# class for Activation layers
class Activation(Layer):
    def __init__(self, activation, input_dims=None, device='CPU'):
        super().__init__(device)
        self.activation = activation
        self.name = 'Activation'
        self._input_dims = input_dims
        self._output_dims = input_dims

    def compute(self):
        raise NotImplementedError("Must override 'compute' by instantiating child of Activation class (Relu, Tanh, Sigmoid, etc...")

    def derivative(self):
        raise NotImplementedError("Must override 'derivative' by instantiating child of Activation class (Relu, Tanh, Sigmoid, etc...")

    def forward(self, input, is_training=True):
        self.input = input
        self.output = self.compute()
        return self.output

    def backward(self, output_gradient, optimizer):
        output = np.multiply(output_gradient, self.derivative())
        return output

class Dense(Layer):
    def __init__(self, output_size, L1_regularizer=0, L2_regularizer=0, input_dims=None, device='CPU', init='Default'):
        super().__init__(device)
        self.name = 'Dense'
        self._input_dims = input_dims
        self._output_dims = output_size
        
        if init == 'Xavier':
            fan_in = input_dims
            fan_out = output_size
            limit = np.sqrt(6.0 / (fan_in + fan_out))
            self.weights = self._xp.random.uniform(-limit, limit, (self._input_dims, self._output_dims))
        elif init == 'He':
            fan_in = input_dims
            self.weights = self._xp.random.randn(self._input_dims, self._output_dims) * np.sqrt(2.0 / fan_in)
        else:
            self.weights = self._xp.random.randn(self._input_dims, self._output_dims)
            
        self.bias = self._xp.random.randn(output_size)
        self.L1_regularizer = L1_regularizer
        self.L2_regularizer = L2_regularizer
    

    def forward(self, input, is_training=True):
        self.input = input
        self.output = self._xp.dot(self.input, self.weights) + self.bias
        return self.output


    def backward(self, output_gradient, optimizer):
        # calculates gradients of weights 'dE/dW', and input 'dE/dX' with respect to error
        self.output_gradient = output_gradient
        self.weight_gradient = self._xp.dot(self.input.T, output_gradient)
        self.bias_gradient = self._xp.sum(self.output_gradient, axis=0)
    
        # add in gradients of regularization wrt weights and bias if regularizers > 0
        # for L1 Regularization
        if self.L1_regularizer >= 0:
            # update weight gradient
            L1_weight_gradient = self._xp.ones_like(self.weights)
            L1_weight_gradient[self.weights<0] = -1
            self.weight_gradient += self.L1_regularizer * L1_weight_gradient

            # update bias gradient
            L1_bias_gradient = self._xp.ones_like(self.bias)
            L1_bias_gradient[self.bias<0] = -1
            self.bias_gradient += self.L1_regularizer * L1_bias_gradient

        # for L2 Regularization
        if self.L2_regularizer >= 0:
            # update weight gradient
            self.weight_gradient += 2 * (self.L2_regularizer * self.weights)

            # update bias gradient
            self.bias_gradient += 2 * (self.L2_regularizer * self.bias)

        input_gradient = self._xp.dot(output_gradient, self.weights.T)

        # update weights and bias
        self.weights, self.bias = optimizer.update_params(self)

        # return input_gradient to be used as output_gradient of previous layer
        return input_gradient


    def get_parameters(self):
        return self.weights, self.bias
    
    def set_parameters(self, weights, bias):
        self.weights = weights
        self.bias = bias
    
    @staticmethod
    def set_input_size(previous_layer):
        """
        Sets the input size of the layer using previous layers output size
        """
        # check to make sure previous layer shape is 2D
        if len(previous_layer.output.shape == 2):
            return previous_layer.output.shape[1] # return second dimension of previous layer output
        else:
            # Throw ERROR that dense required 2D input data
            print(f'Dense layer requires 2D data as input; passed {len(previous_layer.output.shape)}D')
        
class Dropout(Layer):
    
    def __init__(self, dropout_rate, input_dims=None, output_size=None, device='CPU'):
        super().__init__(device)
        self.name = 'Dropout'
        self.keep_rate = 1 - dropout_rate
        self._input_dims = input_dims
        self._output_dims = output_size
        
    def forward(self, input, is_training=True):
        self.input = input
        
        # skip dropout and just output previous layers output if model is being evaluated
        if not is_training:
            self.output = self.input.copy()
            return self.output
        
        self.mask = self._xp.random.binomial(1, self.keep_rate, input.shape) / self.keep_rate
        self.output = input * self.mask
        return self.output
    
    def backward(self, output_gradient, optimizer):
        self.output_gradient = output_gradient
        return self.output_gradient * self.mask
    
class Convolutional(Layer):
    def __init__(self, kernel_size, depth, mode, input_dims=None, device='CPU', init='default'):
        super().__init__(device)
        self.name = 'Convolutional'
        _input_depth, _input_height, _input_width = input_dims
        self._depth = depth
        self._input_dims = input_dims
        self._input_depth = _input_depth
        self._kernels_shape = (self._depth, self._input_depth, kernel_size, kernel_size)
        
        # Compute fan-in and fan-out for the convolution
        fan_in = _input_depth * kernel_size * kernel_size
        fan_out = depth * kernel_size * kernel_size
        
        if init == 'Xavier':
            limit = np.sqrt(6.0 / (fan_in + fan_out))
            self.weights = self._xp.random.uniform(-limit, limit, self._kernels_shape)
        elif init == 'He':
            self.weights = self._xp.random.randn(*self._kernels_shape) * np.sqrt(2.0 / fan_in)
        else:
            self.weights = self._xp.random.randn(*self._kernels_shape)
        
        self.mode = mode
        if self.mode == 'valid':
            self._output_dims = (self._depth, _input_height - kernel_size + 1, _input_width - kernel_size + 1)
        elif self.mode == 'same':
            self._output_dims = (self._depth, _input_height, _input_width)
            
        self.bias = self._xp.random.randn(self._depth)

    def compute_asymmetric_padding(self):
        kernel_size = self._kernels_shape[2]
        pad_total = kernel_size - 1
        pad_before = pad_total // 2
        pad_after = pad_total - pad_before
        return (pad_before, pad_after)

    def forward(self, input, is_training=True):
        self.input = input
        if self.mode == 'same':
            pad_tuple = self.compute_asymmetric_padding()
            self.pad = (pad_tuple, pad_tuple)  # For both height and width
        else:
            self.pad = 0
        self.output = self.conv_forward_im2col(self.input, self.weights, self.bias, stride=1, pad=self.pad)
        return self.output

    def backward(self, output_gradient, optimizer):
        self.output_gradient = output_gradient
        self.weight_gradient, input_gradient = self.conv_backward_im2col(
            self.output_gradient, self.input, self.weights, stride=1, pad=self.pad
        )
        # Compute bias gradient by summing over the batch and spatial dimensions.
        self.bias_gradient = self._xp.sum(self.output_gradient, axis=(0, 2, 3))

        self.weights, self.bias = optimizer.update_params(self)
        return input_gradient

    def conv_forward_im2col(self, x, W, b, stride=1, pad=0):
        N, C, H, W_in = x.shape
        F, _, filter_h, filter_w = W.shape
        if isinstance(pad, int):
            total_pad_h = total_pad_w = 2 * pad
        else:
            pad_h, pad_w = pad
            total_pad_h = sum(pad_h)
            total_pad_w = sum(pad_w)
        out_h = (H + total_pad_h - filter_h) // stride + 1
        out_w = (W_in + total_pad_w - filter_w) // stride + 1
        col = im2col(x, filter_h, filter_w, stride, pad, xp=self._xp)
        W_col = W.reshape(F, -1)
        out = self._xp.dot(col, W_col.T) + b.reshape(1, -1)
        out = out.reshape(N, out_h, out_w, F).transpose(0, 3, 1, 2)
        return out

    def conv_backward_im2col(self, dout, x, W, stride=1, pad=0):
        N, F, out_h, out_w = dout.shape
        dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, F)
        _, C, H, W_in = x.shape
        filter_h, filter_w = W.shape[2], W.shape[3]
        col = im2col(x, filter_h, filter_w, stride, pad, xp=self._xp)
        dW_col = self._xp.dot(dout_reshaped.T, col)
        dW = dW_col.reshape(W.shape)
        W_col = W.reshape(F, -1)
        dcol = self._xp.dot(dout_reshaped, W_col)
        dx = col2im(dcol, x.shape, filter_h, filter_w, stride, pad, xp=self._xp)
        return dW, dx
    
    def get_parameters(self):
        return self.weights, self.bias

    def set_parameters(self, weights, bias):
        if weights.shape == self.weights.shape:
            self.weights = weights
        else:
            print("weights don't match kernel shape")

        if bias.shape == self.bias.shape:
            self.bias = bias
        else:
            print("bias doesn't match bias shape")
    
class Flatten(Layer):
    """
    Class used for flattening the input to an NxD dimension going forward, and restoring data
    to it's previous shape when going backward (back-propogating)
    """
    
    def __init__(self, input_dims=None, device='CPU'):
        super().__init__(device)
        self.name = 'Flatten'
        self._input_dims = input_dims
        self._output_dims = 1
        for i in self._input_dims:
            self._output_dims*=i
        
    def forward(self, input, is_training=True):
        self.input_shape = input.shape
        self.output_shape = (self.input_shape[0], self._output_dims)
        return self._xp.reshape(input, self.output_shape)
    
    def backward(self, output_gradient, optimizer):
        return self._xp.reshape(output_gradient, self.input_shape)
    
class Pool(Layer):
    
    def __init__(self, pool_size=2, stride=2, method='max', input_dims=None, device='CPU'):
        super().__init__(device)
        self.name = 'Pool'
        self.pool_size = pool_size
        self.stride = stride
        self.method = method
        self._input_dims = input_dims
        _out_height = 1 + (self._input_dims[1] - self.pool_size) // self.stride
        _out_width = 1 + (self._input_dims[2] - self.pool_size) // self.stride
        self._output_dims = (self._input_dims[0], _out_height, _out_width)
        
    def forward(self, input):
        self.input = input
        # Recompute output dimensions from the actual input shape
        N, C, H, W = self.input.shape
        out_height = 1 + (H - self.pool_size) // self.stride
        out_width  = 1 + (W - self.pool_size) // self.stride
        _out_shape = (N, C, out_height, out_width)
        self.output = self._xp.zeros(_out_shape)
        
        for height in range(out_height):
            for width in range(out_width):
                height_start = height * self.stride
                height_end = height_start + self.pool_size
                width_start = width * self.stride
                width_end = width_start + self.pool_size
                
                if self.method == 'max':
                    self.output[:, :, height, width] = self._xp.max(input[:, :, height_start:height_end, width_start:width_end], axis=(2,3))
                elif self.method == 'average':
                    self.output[:, :, height, width] = self._xp.mean(input[:, :, height_start:height_end, width_start:width_end], axis=(2,3))
        
        return self.output
    
    def backward(self, output_gradient, optimizer):
        self.output_gradient = output_gradient
        N, C, H, W = self.input.shape
        out_height = self.output.shape[2]
        out_width  = self.output.shape[3]
        input_gradient = self._xp.zeros(self.input.shape)
        
        for n in range(N):
            for d in range(C):
                for height in range(out_height):
                    for width in range(out_width):
                        height_start = height * self.stride
                        height_end = height_start + self.pool_size
                        width_start = width * self.stride
                        width_end = width_start + self.pool_size
                        
                        # Extract the pooling region
                        region = self.input[n, d, height_start:height_end, width_start:width_end]
                        # Compute the maximum value in the region
                        max_val = np.max(region)
                        # Get the indices where the region equals the maximum
                        height_idx, width_idx = self._xp.where(region == max_val)
                        
                        if height_idx.size == 0 or width_idx.size == 0:
                            # If this window is empty for some reason, skip it
                            continue
                        
                        # Use the first index (this is common in max pooling backward)
                        input_gradient[n, d, height_start:height_end, width_start:width_end][height_idx[0], width_idx[0]] = self.output_gradient[n, d, height, width]
        
        return input_gradient


class BatchNormalization(Layer):
    def __init__(self, momentum=0.9, epsilon=0.001, input_dims=None, device='CPU'):
        super().__init__(device)
        self._input_dims = input_dims
        self._output_dims = input_dims
        self.num_features = input_dims[0] if isinstance(input_dims, tuple) else input_dims
        self.momentum = momentum
        self.epsilon = epsilon
        # Initialize gamma (scale) as weight and beta (shift) as bias
        # (Initialized with weight and bias names for standardized names
        # to work with optimizer class)
        self.weights = self._xp.ones(self.num_features)
        self.bias = self._xp.zeros(self.num_features)
        # Running mean and variance for inference
        self.running_mean = self._xp.zeros(self.num_features)
        self.running_var = self._xp.ones(self.num_features)
        self.batch_mean = None
        self.batch_var = None
        self.name = 'BatchNorm'

    def forward(self, x, is_training=True):
        # x is assumed to be shape (N, C, H, W) for conv layers,
        # or (N, D) for dense layers.
        if x.ndim == 4:
            # For convolutional layers, compute mean and variance per channel over N, H, W
            if is_training:
                mean = self._xp.mean(x, axis=(0, 2, 3))
                var = self._xp.var(x, axis=(0, 2, 3))
                # Save mean and var as attributes to be used in backwards pass
                self.batch_mean = mean
                self.batch_var = var
                # Update running stats
                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean
                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var
            else:
                mean = self.running_mean
                var = self.running_var
            # Reshape for broadcasting: (1, C, 1, 1)
            mean = mean.reshape((1, -1, 1, 1))
            var = var.reshape((1, -1, 1, 1))
            gamma = self.weights.reshape((1, -1, 1, 1))
            beta = self.bias.reshape((1, -1, 1, 1))
        else:
            # For dense layers (shape: (N, D))
            if is_training:
                mean = self._xp.mean(x, axis=0)
                var = self._xp.var(x, axis=0)
                # Save mean and var as attributes to be used in backwards pass
                self.batch_mean = mean
                self.batch_var = var
                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean
                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var
            else:
                mean = self.running_mean
                var = self.running_var
            gamma = self.weights
            beta = self.bias

        # Normalize and then scale and shift
        self.normalized = (x - mean) / self._xp.sqrt(var + self.epsilon)
        self.output = gamma * self.normalized + beta
        return self.output

    def backward(self, output_gradient, optimizer):
        self.output_gradient = output_gradient
        if self.output_gradient.ndim == 2:
            # Dense case: x shape (N, D)
            N, D = self.output_gradient.shape
            dgamma = self._xp.sum(self.output_gradient * self.normalized, axis=0)
            dbeta = self._xp.sum(self.output_gradient, axis=0)
            dnormalized = self.output_gradient * self.weights  # (N, D)
            std_inv = 1.0 / self._xp.sqrt(self.batch_var + self.epsilon)
            input_gradient = (1.0 / N) * std_inv * (N * dnormalized - self._xp.sum(dnormalized, axis=0)
                                        - self.normalized * self._xp.sum(dnormalized * self.normalized, axis=0))
        else:
            # Convolutional case: x shape (N, C, H, W)
            N, C, H, W = self.output_gradient.shape
            dgamma = self._xp.sum(self.output_gradient * self.normalized, axis=(0, 2, 3))
            dbeta = self._xp.sum(self.output_gradient, axis=(0, 2, 3))
            dnormalized = self.output_gradient * self.weights.reshape((1, C, 1, 1))
            std_inv = 1.0 / self._xp.sqrt(self.batch_var + self.epsilon)
            std_inv = std_inv.reshape((1, C, 1, 1))
            # Total number of elements per channel
            M = N * H * W
            input_gradient = (1.0 / M) * std_inv * (M * dnormalized - self._xp.sum(dnormalized, axis=(0,2,3), keepdims=True)
                                        - self.normalized * self._xp.sum(dnormalized * self.normalized, axis=(0,2,3), keepdims=True))
        
        self.weight_gradient = dgamma
        self.bias_gradient = dbeta
        # update weights and bias using optimizer
        self.weights, self.bias = optimizer.update_params(self)
        return input_gradient

