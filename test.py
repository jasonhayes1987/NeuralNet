# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E-nGuv2IK2pOofzzMZkxzNMEwiVxn5o_
"""

import sys
from google.colab import drive
drive.mount('/content/drive')
sys.path.append('/content/drive/MyDrive/Colab Notebooks/NeuralNet')
import network as Network
import numpy as np
from tensorflow.keras import activations
from tensorflow.keras import backend
import tensorflow as tf
import unittest

class UnetTest(unittest.TestCase):

    def setUp(self):
        super(UnetTest, self).setUp()

    def tearDown(self):
        pass

    def test_All(self):
        self.test_Layers()
        self.test_Activations()
        self.test_Losses()

    def test_Layers(self):
        self.test_Activations()
        self.test_Dense()
        self.test_Convolutional()
        self.test_Pool()
        self.test_Flatten()

    def test_Activations(self):
        self.test_relu()
        self.test_elu()
        self.test_gelu()
        self.test_tanh()
        self.test_sigmoid()
        self.test_softmax()

    def test_Losses(self):
        self.test_MSE()
        self.test_Sparse_Categorical_Cross_Entropy()

    def test_relu(self):
        test_arr = np.random.randn(5,5)
        tf_out = np.array(activations.relu(test_arr))
        
        net = Network.Neural_Network()
        net.compile_model()
        net.add_Activation('relu', input_dims=test_arr.shape)
        net_out = net.layers[0].forward(test_arr)

        # self.assertEqual(tf_out.all(), net_out.all())
        if np.testing.assert_allclose(tf_out, net_out, rtol=1e-05) == None:
            print('Relu.forward() passed')

        x = tf.Variable(test_arr)
        with tf.GradientTape() as tape:
            tf_out = activations.relu(x)
        tf_dx = tape.gradient(tf_out, x).numpy()

        net_dx = net.layers[0].derivative()

        if np.testing.assert_allclose(tf_dx, net_dx, rtol=1e-05) == None:
            print('Relu.derivative() passed')

    def test_tanh(self):
        test_arr = np.random.randn(5,5)
        tf_out = np.array(activations.tanh(test_arr))

        net = Network.Neural_Network()
        net.compile_model()
        net.add_Activation('tanh', input_dims=test_arr.shape)
        net_out = net.layers[0].forward(test_arr)

        if np.testing.assert_allclose(tf_out, net_out, rtol=1e-05) == None:
            print('Tanh.forward() passed')

        x = tf.Variable(test_arr)
        with tf.GradientTape() as tape:
            tf_out = activations.tanh(x)
        tf_dx = tape.gradient(tf_out, x).numpy()

        net_dx = net.layers[0].derivative()

        if np.testing.assert_allclose(tf_dx, net_dx, rtol=1e-05) == None:
            print('Tanh.derivative() passed')

    def test_softmax(self):
        test_arr = np.random.randn(5,5)
        tf_test_arr = tf.Variable(test_arr)
        tf_out = np.array(activations.softmax(tf_test_arr))

        net = Network.Neural_Network()
        net.compile_model()
        net.add_Activation('softmax', input_dims=test_arr.shape)
        net_out = net.layers[0].forward(test_arr)
        
        if np.testing.assert_allclose(tf_out, net_out, rtol=1e-05) == None:
            print('softmax.forward() passed')

        with tf.GradientTape() as tape:
            tf_out = activations.softmax(tf_test_arr)
        tf_dx = tape.gradient(tf_out, tf_test_arr).numpy()

        net_dx = net.layers[0].backward(np.ones_like(net_out), optimizer=None)

        if np.testing.assert_allclose(tf_dx, net_dx, rtol=0, atol=1e-10) == None:
            print('softmax.backward() passed')

    def test_elu(self):
        test_arr = np.random.randn(5,5)
        tf_test_arr = tf.Variable(test_arr)
        tf_out = np.array(activations.elu(tf_test_arr))

        net = Network.Neural_Network()
        net.compile_model()
        net.add_Activation('elu', input_dims=test_arr.shape)
        net_out = net.layers[0].forward(test_arr)

        if np.testing.assert_allclose(tf_out, net_out, rtol=1e-05) == None:
            print('Elu.forward() passed')

        with tf.GradientTape() as tape:
            tf_out = activations.elu(tf_test_arr)
        tf_dx = tape.gradient(tf_out, tf_test_arr).numpy()

        net_dx = net.layers[0].derivative()

        if np.testing.assert_allclose(tf_dx, net_dx, rtol=1e-05) == None:
            print('Elu.derivative() passed')


    def test_gelu(self):
        test_arr = np.random.randn(5,5)
        tf_test_arr = tf.Variable(test_arr)
        tf_out = np.array(activations.gelu(tf_test_arr))

        net = Network.Neural_Network()
        net.compile_model()
        net.add_Activation('gelu', input_dims=test_arr.shape)
        net_out = net.layers[0].forward(test_arr)

        if np.testing.assert_allclose(tf_out, net_out, rtol=1e-05) == None:
            print('Gelu.forward() passed')

        with tf.GradientTape() as tape:
            tf_out = activations.gelu(tf_test_arr)
        tf_dx = tape.gradient(tf_out, tf_test_arr).numpy()

        net_dx = net.layers[0].derivative()

        if np.testing.assert_allclose(tf_dx, net_dx, rtol=1e-04) == None:
            print('Gelu.derivative() passed')


    def test_sigmoid(self):
        test_arr = np.random.randn(5,5)
        tf_test_arr = tf.Variable(test_arr)
        tf_out = np.array(activations.sigmoid(tf_test_arr))

        net = Network.Neural_Network()
        net.compile_model()
        net.add_Activation('sigmoid', input_dims=test_arr.shape)
        net_out = net.layers[0].forward(test_arr)

        if np.testing.assert_allclose(tf_out, net_out, rtol=1e-05) == None:
            print('sigmoid.forward() passed')

        with tf.GradientTape() as tape:
            tf_out = activations.sigmoid(tf_test_arr)
        tf_dx = tape.gradient(tf_out, tf_test_arr).numpy()

        net_dx = net.layers[0].derivative()

        # self.assertEqual(tf_dx.all(), net_dx.all())
        if np.testing.assert_allclose(tf_dx, net_dx, rtol=1e-05) == None:
            print('sigmoid.derivative() passed')


    def test_Dense(self):
        data = np.float32(np.random.randn(5,5))
        tf_data = tf.Variable(data, dtype=tf.float32)

        tf_model = tf.keras.layers.Dense(5, name="dense_1")


        tf_out = tf_model(tf_data)

        tf_weights = tf_model.weights
        weights = tf_weights[0].numpy()
        bias = tf_weights[1].numpy()

        optimizer = Network.Optimizers.Adam(learning_rate=0.01, decay=1e-2)
        net = Network.Neural_Network()
        net.compile_model(optimizer=optimizer)
        net.add_Dense(5,input_dims=data.shape[1])
        net.layers[0].set_parameters(weights, bias)
        net_out = net.layers[0].forward(data)

        if np.testing.assert_allclose(net_out,tf_out, rtol=1e-05) == None:
            print('Dense.forward() passed')

        with tf.GradientTape() as tape:
            tf_out = tf_model(tf_data)
        tf_dx = tape.gradient(tf_out, tf_data).numpy()

        net_dx = net.layers[0].backward(output_gradient=np.ones_like(net.layers[0].input), optimizer=net.optimizer)

        if np.testing.assert_allclose(tf_dx, net_dx, rtol=1e-05) == None:
            print('Dense.backward() passed')


    def test_Convolutional(self):
        data = np.float32(np.random.randn(2,3,5,5))

        tf_data = tf.Variable(data, dtype=tf.float32)

        tf_model = tf.keras.layers.Conv2D(2,3, data_format='channels_first', input_shape=data.shape[1:], name='Conv_1')
        y = tf_model(tf_data)

        weights = tf.transpose(tf_model.kernel, [3,2,0,1]).numpy()

        optimizer = Network.Optimizers.Adam(learning_rate=0.01, decay=1e-2)
        net = Network.Neural_Network()
        net.compile_model(optimizer=optimizer)
        net.add_Convolutional(3, 2, mode='valid', input_dims=data.shape[1:])
        bias = np.zeros(shape=net.layers[0]._output_dims)
        net.layers[0].set_parameters(weights, bias)
        net_out = net.layers[0].forward(data)

        if np.testing.assert_allclose(net_out,y, rtol=0, atol=1e-5) == None:
            print('Convolution.forward() passed')

        with tf.GradientTape() as tape:
            tf_out = tf_model(tf_data)
        tf_dx = tape.gradient(tf_out, tf_data).numpy()

        net_dx = net.layers[0].backward(output_gradient=np.ones_like(net.layers[0].output), optimizer=net.optimizer)

        if np.testing.assert_allclose(tf_dx, net_dx, rtol=1e-05) == None:
            print('Convolutional.backward() passed')


    def test_Pool(self):
        data = np.float32(np.random.randn(1,2,10,10))

        tf_data = tf.Variable(data, dtype=tf.float32)

        tf_model = tf.keras.layers.MaxPool2D(data_format='channels_first')
        y = tf_model(tf_data)

        optimizer = Network.Optimizers.Adam(learning_rate=0.01, decay=1e-2)
        net = Network.Neural_Network()
        net.compile_model(optimizer=optimizer)
        net.add_Pool(input_dims=data.shape[1:])

        net_out = net.layers[0].forward(data)

        if np.testing.assert_allclose(net_out,y, rtol=1e-05) == None:
            print('Pool.forward() passed')

        with tf.GradientTape() as tape:
            tf_out = tf_model(tf_data)
        tf_dx = tape.gradient(tf_out, tf_data).numpy()

        net_dx = net.layers[0].backward(output_gradient=np.ones_like(net.layers[0].output), optimizer=net.optimizer)

        if np.testing.assert_allclose(tf_dx, net_dx, rtol=1e-05) == None:
            print('Pool.backward() passed')


    def test_Flatten(self):
        shape = (10,8,5,5)
        data = np.zeros(shape)

        optimizer = Network.Optimizers.Adam(learning_rate=0.01, decay=1e-2)
        net = Network.Neural_Network()
        net.compile_model(optimizer=optimizer)
        net.add_Flatten(input_dims=data.shape[1:])

        net_out = net.layers[0].forward(data)

        y = np.zeros((10,8*5*5))

        if np.testing.assert_allclose(net_out,y, rtol=1e-05) == None:
            print('Flatten.forward() passed')

        net_back = net.layers[0].backward(y, optimizer)

        if np.testing.assert_allclose(net_back,data, rtol=1e-05) == None:
            print('Flatten.bacward() passed')


    def test_MSE(self):
        preds = np.random.randn(10)
        true = np.random.randn(10)
        tf_preds = tf.Variable(preds)
        tf_true = tf.Variable(true)

        mse = Network.Losses.Mean_Squared_Error()

        net_loss = mse.calculate(true,preds)

        tf_mse = tf.keras.losses.MeanSquaredError()
        tf_loss = tf_mse(true,preds).numpy()

        if np.testing.assert_allclose(net_loss, tf_loss, rtol=1e-05) == None:
            print('Mean_Squared_Error.calculate() passed')

        net_loss_dx = mse.backward(true,preds)

        with tf.GradientTape() as tape:
            tf_loss = tf_mse(tf_true,tf_preds)
        tf_loss_dx = tape.gradient(tf_loss, tf_preds).numpy()

        if np.testing.assert_allclose(net_loss_dx, tf_loss_dx, rtol=1e-05) == None:
            print('Mean_Squared_Error.backward() passed')


    def test_Sparse_Categorical_Cross_Entropy(self):
        preds = np.random.randn(10,5)
        probs = activations.softmax(tf.Variable(preds)).numpy()
        targets = np.random.randint(0,5,size=10)
        tf_probs = tf.Variable(probs, dtype=tf.float32)
        tf_targets = tf.Variable(targets, dtype=tf.float32)

        scce = Network.Losses.Sparse_Categorical_Cross_Entropy()
        net_loss = scce.calculate(targets,probs)

        tf_scce = tf.keras.losses.SparseCategoricalCrossentropy()
        tf_loss = tf_scce(tf_targets,tf_probs).numpy()

        if np.testing.assert_allclose(net_loss, tf_loss, rtol=1e-05) == None:
            print('Mean_Squared_Error.calculate() passed')

        net_loss_dx = scce.backward(targets,probs)

        with tf.GradientTape() as tape:
            tf_loss = tf_scce(tf_targets,tf_probs)
        tf_loss_dx = tape.gradient(tf_loss, tf_probs)

        if np.testing.assert_allclose(net_loss_dx, tf_loss_dx - 0.1, rtol=0, atol=1e-5) == None:
            print('Mean_Squared_Error.backward() passed')